{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dbbab9e-648f-4f68-aced-13b44c810edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries and NLTK resources have been imported/downloaded.\n",
      "1.12.1.post200\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hanif/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/hanif/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626f6472-af4e-4c81-80f4-2ec0c047a53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries and NLTK resources have been imported/downloaded.\n",
      "PyTorch Version: 1.12.1.post200\n",
      "CUDA Available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hanif/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/hanif/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# For Text Preprocessing (NLTK)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize # Kita akan memodifikasi cara ini digunakan\n",
    "from nltk.corpus import stopwords # Ini hanya untuk NLTK stopwords, tapi kita pakai Sastrawi\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer # Mungkin tidak terpakai untuk Indo, tapi tetap diimpor\n",
    "\n",
    "# For Text Preprocessing (Sastrawi for Indonesian language stemming)\n",
    "# Make sure you've installed it: pip install Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# For Transformer Models (MedicalBERT/XLM-RoBERTa)\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# For Keyword Extraction (TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For Word Embeddings (Gensim)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# --- Download NLTK Resources ---\n",
    "# Kita tetap perlu download beberapa NLTK resources, terutama 'punkt'\n",
    "# agar word_tokenize (walaupun tidak spesifik Indo) dapat berjalan tanpa LookupError\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/omw-1.4') # Open Multilingual Wordnet\n",
    "except LookupError:\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "print(\"All libraries and NLTK resources have been imported/downloaded.\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\"); print(f\"CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6760bd-837b-49a9-9d41-d5afa58444db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook saat ini berjalan di direktori: /home/hanif/ds-ml-projects/medical_chatbot/notebooks\n",
      "\n",
      "Isi dari direktori saat ini:\n",
      "- medical_entity_detection.ipynb\n",
      "- .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Mendapatkan current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "print(f\"Notebook saat ini berjalan di direktori: {current_directory}\")\n",
    "\n",
    "# Anda juga bisa mencantumkan isi direktori tersebut\n",
    "print(\"\\nIsi dari direktori saat ini:\")\n",
    "for item in os.listdir(current_directory):\n",
    "    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a53bf6-f575-4667-a879-a81fdbeb4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Number of rows and columns: (100, 2)\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "                                            penyakit  \\\n",
      "0                     menggigil, demam, sakit kepala   \n",
      "1  Kaku kuduk, penurunan kesadaran, muntah proyek...   \n",
      "2  Mata lengket, mata berair, pandangan sedikit k...   \n",
      "3   Pipi bengkak, nyeri saat mengunyah, nyeri testis   \n",
      "4        Gusi bengkak, gusi kemerahan, gusi berdarah   \n",
      "\n",
      "                              diagnosis  \n",
      "0              Malaria (bentuk benigma)  \n",
      "1  Meningitis + perdarahan subarachnoid  \n",
      "2                                   NaN  \n",
      "3                             Parotitis  \n",
      "4                                   NaN  \n",
      "\n",
      "Dataset column information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   penyakit   100 non-null    object\n",
      " 1   diagnosis  93 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.7+ KB\n",
      "\n",
      "Dataset column names:\n",
      "Index(['penyakit', 'diagnosis'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '../data/gejala_penyakit/data_penyakit.csv'\n",
    "\n",
    "# Verify file existence before attempting to load it\n",
    "if os.path.exists(dataset_path):\n",
    "    try:\n",
    "        df_penyakit = pd.read_csv(dataset_path)\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Number of rows and columns: {df_penyakit.shape}\")\n",
    "        print(\"\\nFirst 5 rows of the dataset:\")\n",
    "        print(df_penyakit.head())\n",
    "        print(\"\\nDataset column information:\")\n",
    "        df_penyakit.info()\n",
    "        print(\"\\nDataset column names:\")\n",
    "        print(df_penyakit.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the dataset: {e}\")\n",
    "else:\n",
    "    print(f\"Error: File not found at {dataset_path}. Please ensure the path and filename are correct.\")\n",
    "    print(f\"Your Current Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005da921-17cb-4d07-9e92-8fe613969bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model at: ../models/medical_bert/model/\n",
      "Loading tokenizer from local path: ../models/medical_bert/model/\n",
      "Loading model from local path: ../models/medical_bert/model/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 10:13:47.698660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully from local path!\n",
      "Model set to evaluation mode.\n",
      "Model moved to GPU.\n"
     ]
    }
   ],
   "source": [
    "# Define the model name to load from Hugging Face Model Hub\n",
    "huggingface_model_name = 'indobenchmark/indobert-base-p1'\n",
    "\n",
    "# Define the local path where the model and tokenizer will be saved\n",
    "# This aligns with our project structure: models/medical_bert/model/\n",
    "local_model_path = '../models/medical_bert/model/'\n",
    "\n",
    "print(f\"Checking for existing model at: {local_model_path}\")\n",
    "\n",
    "try:\n",
    "    # Check if the model already exists locally\n",
    "    if os.path.exists(local_model_path) and os.listdir(local_model_path):\n",
    "        print(f\"Loading tokenizer from local path: {local_model_path}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "        print(f\"Loading model from local path: {local_model_path}\")\n",
    "        model = AutoModelForTokenClassification.from_pretrained(local_model_path)\n",
    "        print(\"Model and tokenizer loaded successfully from local path!\")\n",
    "    else:\n",
    "        print(f\"Local model not found. Downloading from Hugging Face: {huggingface_model_name}\")\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(local_model_path, exist_ok=True)\n",
    "\n",
    "        # Load the tokenizer from Hugging Face\n",
    "        tokenizer = AutoTokenizer.from_pretrained(huggingface_model_name)\n",
    "        # Load the model from Hugging Face\n",
    "        model = AutoModelForTokenClassification.from_pretrained(huggingface_model_name)\n",
    "\n",
    "        print(f\"Saving tokenizer to local path: {local_model_path}\")\n",
    "        tokenizer.save_pretrained(local_model_path)\n",
    "        print(f\"Saving model to local path: {local_model_path}\")\n",
    "        model.save_pretrained(local_model_path)\n",
    "        print(\"Model and tokenizer downloaded and saved locally!\")\n",
    "\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    print(\"Model set to evaluation mode.\")\n",
    "\n",
    "    # Check if GPU is available and move model to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "        print(\"Model moved to GPU.\")\n",
    "    else:\n",
    "        print(\"GPU not available, model running on CPU.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading or saving the model/tokenizer: {e}\")\n",
    "    print(\"Please ensure you have an internet connection if downloading for the first first time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5251d2-3ce5-4cc2-8965-e937f139ba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating or updating stopwords_id.txt file...\n",
      "File stopwords_id.txt successfully created/updated at: ../data/stopwords_id.txt\n",
      "\n",
      "Starting text preprocessing for 'df_penyakit' using Sastrawi...\n",
      "Columns 'gejala_processed' (from 'penyakit' column), 'diagnosis_processed' (from 'diagnosis' column) created.\n",
      "\n",
      "Preprocessing complete! Displaying first 5 rows with new processed columns:\n",
      "                                            penyakit  \\\n",
      "0                     menggigil, demam, sakit kepala   \n",
      "1  Kaku kuduk, penurunan kesadaran, muntah proyek...   \n",
      "2  Mata lengket, mata berair, pandangan sedikit k...   \n",
      "3   Pipi bengkak, nyeri saat mengunyah, nyeri testis   \n",
      "4        Gusi bengkak, gusi kemerahan, gusi berdarah   \n",
      "\n",
      "                              diagnosis  \\\n",
      "0              Malaria (bentuk benigma)   \n",
      "1  Meningitis + perdarahan subarachnoid   \n",
      "2                                   NaN   \n",
      "3                             Parotitis   \n",
      "4                                   NaN   \n",
      "\n",
      "                                    gejala_processed  \\\n",
      "0                           gigil demam sakit kepala   \n",
      "1  kaku kuduk turun sadar muntah proyektil sakit ...   \n",
      "2  mata lengket mata air pandang sedikit kabur ke...   \n",
      "3             pipi bengkak nyeri kunyah nyeri testis   \n",
      "4                 gusi bengkak gusi merah gusi darah   \n",
      "\n",
      "             diagnosis_processed  \n",
      "0         malaria bentuk benigma  \n",
      "1  meningitis darah subarachnoid  \n",
      "2                                 \n",
      "3                      parotitis  \n",
      "4                                 \n",
      "\n",
      "Checking info of processed columns:\n",
      "\n",
      "Info for 'gejala_processed':\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Series name: gejala_processed\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "100 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 928.0+ bytes\n",
      "\n",
      "Info for 'diagnosis_processed':\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Series name: diagnosis_processed\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "100 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 928.0+ bytes\n",
      "\n",
      "Processed DataFrame has been saved to: ../data/gejala_penyakit/data_penyakit_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Ensure 'data' Folder Exists and Create/Update stopwords_id.txt ---\n",
    "# This part runs once to prepare the stopwords file on disk.\n",
    "print(\"Creating or updating stopwords_id.txt file...\")\n",
    "\n",
    "# Initialize StopWordRemoverFactory to get the default stopwords list\n",
    "stopword_factory_for_file = StopWordRemoverFactory()\n",
    "stopwords_list_for_file = stopword_factory_for_file.get_stop_words() # Assign the list here\n",
    "\n",
    "# Define the path for the data folder (assuming notebook is in project root)\n",
    "data_folder_path = '../data'\n",
    "stopwords_filepath = os.path.join(data_folder_path, 'stopwords_id.txt')\n",
    "\n",
    "# Create the 'data' folder if it doesn't exist\n",
    "os.makedirs(data_folder_path, exist_ok=True)\n",
    "\n",
    "# Save the stopwords list to the file\n",
    "with open(stopwords_filepath, 'w', encoding='utf-8') as f:\n",
    "    for word in stopwords_list_for_file:\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "print(f\"File stopwords_id.txt successfully created/updated at: {stopwords_filepath}\")\n",
    "\n",
    "# --- Step 2: Initialize Sastrawi's Stemmer and Load Stopwords for Preprocessing ---\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "# Load the stopwords from the file we just created/updated.\n",
    "# This ensures consistency, even if the notebook is restarted.\n",
    "with open(stopwords_filepath, 'r', encoding='utf-8') as f:\n",
    "    stopwords_id_for_preprocessing = f.read().splitlines()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function to preprocess text:\n",
    "    1. Lowercasing\n",
    "    2. Remove non-alphanumeric characters (keep spaces)\n",
    "    3. Tokenization\n",
    "    4. Stopword removal (using the loaded list)\n",
    "    5. Stemming (using Sastrawi)\n",
    "    6. Filter out single-character or purely digit tokens\n",
    "    7. Join tokens back into a string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\" # Handle non-string input, e.g., NaN values\n",
    "\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove special characters, keep alphanumeric and spaces\n",
    "\n",
    "    tokens = text.split() # Split the string by spaces\n",
    "\n",
    "    # Apply stopword removal and stemming in one go\n",
    "    processed_tokens = [\n",
    "        stemmer.stem(token) for token in tokens\n",
    "        if token not in stopwords_id_for_preprocessing # Filter out stop words\n",
    "        and len(token) > 1 and not token.isdigit() # Filter single-character and purely digit tokens\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(processed_tokens) # Join tokens back to a string\n",
    "\n",
    "print(\"\\nStarting text preprocessing for 'df_penyakit' using Sastrawi...\")\n",
    "\n",
    "# --- Step 3: Apply Preprocessing to DataFrame Columns ---\n",
    "processed_columns_info = []\n",
    "\n",
    "# Process 'gejala' column if it exists\n",
    "if 'penyakit' in df_penyakit.columns: # Using 'gejala' as the original input column name\n",
    "    df_penyakit['gejala_processed'] = df_penyakit['penyakit'].apply(preprocess_text)\n",
    "    processed_columns_info.append(\"'gejala_processed' (from 'penyakit' column)\")\n",
    "else:\n",
    "    print(\"Warning: 'penyakit' column not found. Please verify your DataFrame column names.\")\n",
    "\n",
    "# Process 'diagnosis' column if it exists\n",
    "if 'diagnosis' in df_penyakit.columns:\n",
    "    df_penyakit['diagnosis_processed'] = df_penyakit['diagnosis'].apply(preprocess_text)\n",
    "    processed_columns_info.append(\"'diagnosis_processed' (from 'diagnosis' column)\")\n",
    "else:\n",
    "    print(\"Warning: 'diagnosis' column not found. Please verify your DataFrame column names.\")\n",
    "\n",
    "if processed_columns_info:\n",
    "    print(f\"Columns {', '.join(processed_columns_info)} created.\")\n",
    "else:\n",
    "    print(\"No relevant columns processed. Please check your DataFrame column names.\")\n",
    "\n",
    "print(\"\\nPreprocessing complete! Displaying first 5 rows with new processed columns:\")\n",
    "print(df_penyakit.head())\n",
    "print(\"\\nChecking info of processed columns:\")\n",
    "for col_name in ['gejala_processed', 'diagnosis_processed']:\n",
    "    if col_name in df_penyakit.columns:\n",
    "        print(f\"\\nInfo for '{col_name}':\")\n",
    "        df_penyakit[col_name].info()\n",
    "\n",
    "\n",
    "# --- Step 4: Save the Processed DataFrame (Crucial for Streamlit) ---\n",
    "# Save the processed DataFrame to a CSV file.\n",
    "# This output path must be consistent with the path that will be read in your Streamlit app.\n",
    "output_path = '../data/gejala_penyakit/data_penyakit_processed.csv' # Ensure this path matches what Streamlit reads\n",
    "df_penyakit.to_csv(output_path, index=False)\n",
    "print(f\"\\nProcessed DataFrame has been saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179eb791-72fe-4b9b-bf98-f5f0f0526814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique symptom words extracted: 389\n",
      "First 10 unique symptom words:\n",
      "['abdomen', 'air', 'akibat', 'aksila', 'alami', 'alis', 'amandel', 'amis', 'ampas', 'anak']\n",
      "\n",
      "Total unique diagnosis words extracted: 122\n",
      "First 10 unique diagnosis words:\n",
      "['abses', 'akuisita', 'akut', 'alergi', 'anak', 'antrax', 'appendicitis', 'bacterial', 'batuk', 'benigma']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of unique processed symptoms\n",
    "# Mengambil semua teks gejala yang sudah diproses, lalu memisahkannya menjadi kata-kata unik.\n",
    "# Flattening the list of lists into a single list of words.\n",
    "all_symptoms_words = []\n",
    "for symptoms_text in df_penyakit['gejala_processed'].dropna(): # DropNa to handle potential NaN values\n",
    "    all_symptoms_words.extend(symptoms_text.split()) # Use split() as we did in preprocessing\n",
    "\n",
    "# Convert to a set to get unique words, then back to a sorted list\n",
    "unique_symptoms = sorted(list(set(all_symptoms_words)))\n",
    "\n",
    "print(f\"Total unique symptom words extracted: {len(unique_symptoms)}\")\n",
    "print(\"First 10 unique symptom words:\")\n",
    "print(unique_symptoms[:10])\n",
    "\n",
    "# Create a list of unique processed diagnoses/diseases\n",
    "# Mirip dengan gejala, kita ambil diagnosis yang sudah diproses.\n",
    "all_diagnosis_words = []\n",
    "for diagnosis_text in df_penyakit['diagnosis_processed'].dropna(): # DropNa to handle potential NaN values\n",
    "    all_diagnosis_words.extend(diagnosis_text.split())\n",
    "\n",
    "unique_diagnosis = sorted(list(set(all_diagnosis_words)))\n",
    "\n",
    "print(f\"\\nTotal unique diagnosis words extracted: {len(unique_diagnosis)}\")\n",
    "print(\"First 10 unique diagnosis words:\")\n",
    "print(unique_diagnosis[:10])\n",
    "\n",
    "# --- (Opsional) Menggabungkan menjadi satu daftar entitas jika diinginkan ---\n",
    "# Ini berguna jika Anda ingin membuat satu daftar besar dari semua entitas medis.\n",
    "# all_medical_entities = sorted(list(set(unique_symptoms + unique_diagnosis)))\n",
    "# print(f\"\\nTotal unique medical entities (symptoms + diagnoses): {len(all_medical_entities)}\")\n",
    "# print(\"First 10 unique medical entities:\")\n",
    "# print(all_medical_entities[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fcad4a6-a823-420a-972a-1edaba6c5aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TF-IDF calculation for symptoms and diagnoses...\n",
      "Shape of TF-IDF matrix for symptoms: (100, 389)\n",
      "Shape of TF-IDF matrix for diagnoses: (100, 122)\n",
      "\n",
      "TF-IDF calculation completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize TfidfVectorizers for symptoms and diagnoses.\n",
    "# min_df=1 ensures terms appearing in at least one document are considered.\n",
    "# max_df=1.0 ensures no upper limit on document frequency (stopwords already handled).\n",
    "tfidf_vectorizer_gejala = TfidfVectorizer(min_df=1, max_df=1.0)\n",
    "tfidf_vectorizer_diagnosis = TfidfVectorizer(min_df=1, max_df=1.0)\n",
    "\n",
    "print(\"Starting TF-IDF calculation for symptoms and diagnoses...\")\n",
    "\n",
    "# Fit and transform 'gejala_processed' column to TF-IDF matrix.\n",
    "# .fillna('') handles potential NaN values by treating them as empty strings.\n",
    "tfidf_matrix_gejala = tfidf_vectorizer_gejala.fit_transform(df_penyakit['gejala_processed'].fillna(''))\n",
    "print(f\"Shape of TF-IDF matrix for symptoms: {tfidf_matrix_gejala.shape}\")\n",
    "\n",
    "# Fit and transform 'diagnosis_processed' column to TF-IDF matrix.\n",
    "tfidf_matrix_diagnosis = tfidf_vectorizer_diagnosis.fit_transform(df_penyakit['diagnosis_processed'].fillna(''))\n",
    "print(f\"Shape of TF-IDF matrix for diagnoses: {tfidf_matrix_diagnosis.shape}\")\n",
    "\n",
    "# Optional: Save vectorizers and matrices if needed for later use/deployment\n",
    "# import joblib\n",
    "# joblib.dump(tfidf_vectorizer_gejala, 'models/word_embeddings/tfidf_vectorizer_gejala.pkl')\n",
    "# joblib.dump(tfidf_matrix_gejala, 'models/word_embeddings/tfidf_matrix_gejala.pkl')\n",
    "\n",
    "print(\"\\nTF-IDF calculation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eebee688-c8e0-4bec-bf99-4fc2892528b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences for Word2Vec training: 200\n",
      "\n",
      "Starting Word2Vec model training...\n",
      "Word2Vec model training completed.\n",
      "Word2Vec model saved to: ../models/word_embeddings/word2vec_medical.model\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for Word2Vec: list of lists of words (tokens)\n",
    "sentences_for_w2v = []\n",
    "for text in df_penyakit['gejala_processed'].dropna():\n",
    "    sentences_for_w2v.append(text.split())\n",
    "for text in df_penyakit['diagnosis_processed'].dropna():\n",
    "    sentences_for_w2v.append(text.split())\n",
    "\n",
    "print(f\"Total sentences for Word2Vec training: {len(sentences_for_w2v)}\")\n",
    "\n",
    "# Train the Word2Vec model\n",
    "# Parameters: vector_size (embedding dimension), window (context window),\n",
    "# min_count (ignore infrequent words), sg (0: CBOW, 1: Skip-gram), epochs.\n",
    "print(\"\\nStarting Word2Vec model training...\")\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=sentences_for_w2v,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=0,\n",
    "    epochs=100\n",
    ")\n",
    "print(\"Word2Vec model training completed.\")\n",
    "\n",
    "# Save the Word2Vec model to the designated models folder\n",
    "model_save_path = '../models/word_embeddings/word2vec_medical.model'\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "word2vec_model.save(model_save_path)\n",
    "print(f\"Word2Vec model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5626d24e-d672-41ae-be71-9297c78564aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Word2Vec model:\n",
      "Words similar to 'demam': [('serviks', 0.9985533356666565), ('sakit', 0.9983733296394348), ('ruam', 0.9982883334159851), ('capai', 0.9981731176376343), ('hari', 0.9981451034545898)]\n"
     ]
    }
   ],
   "source": [
    "# Optional: Test the Word2Vec model (uncomment to run)\n",
    "print(\"\\nTesting Word2Vec model:\")\n",
    "try:\n",
    "    similar_words = word2vec_model.wv.most_similar('demam', topn=5)\n",
    "    print(f\"Words similar to 'demam': {similar_words}\")\n",
    "except KeyError:\n",
    "    print(\"Word 'demam' not in model vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c7e231d-9f38-4809-8a7f-46781fa47b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fungsi deteksi entitas medis `detect_medical_entities` telah didefinisikan.\n",
      "\n",
      "--- Contoh Deteksi Entitas ---\n",
      "Input: 'saya mengalami demam tinggi dan sakit kepala yang hebat sekali'\n",
      "Gejala terdeteksi: ['demam', 'hebat', 'kepala', 'sakit', 'tinggi']\n",
      "Diagnosis terdeteksi: ['sakit']\n",
      "\n",
      "Input: 'kaku kuduk dan penurunan kesadaran itu adalah gejala meningitis'\n",
      "Gejala terdeteksi: ['gejala', 'kaku', 'kuduk']\n",
      "Diagnosis terdeteksi: ['meningitis']\n"
     ]
    }
   ],
   "source": [
    "def detect_medical_entities(text_input, symptom_list, diagnosis_list):\n",
    "    \"\"\"\n",
    "    Detects known medical symptoms and diagnoses in a given text input.\n",
    "    The input text is preprocessed for better matching.\n",
    "\n",
    "    Args:\n",
    "        text_input (str): The raw text input from the user.\n",
    "        symptom_list (list): A list of unique preprocessed symptom words.\n",
    "        diagnosis_list (list): A list of unique preprocessed diagnosis words.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "               - detected_symptoms (list of str): List of detected symptom words.\n",
    "               - detected_diagnoses (list of str): List of detected diagnosis words.\n",
    "    \"\"\"\n",
    "    if not isinstance(text_input, str):\n",
    "        return [], []\n",
    "\n",
    "    # Preprocess the input text using the same logic as df_penyakit\n",
    "    # Lowercasing and removing non-alphanumeric characters\n",
    "    processed_input = text_input.lower()\n",
    "    processed_input = re.sub(r'[^a-z0-9\\s]', '', processed_input)\n",
    "\n",
    "    # Tokenize the processed input\n",
    "    # Using simple split as done for df_penyakit preprocessing\n",
    "    input_tokens = processed_input.split()\n",
    "\n",
    "    detected_symptoms = []\n",
    "    detected_diagnoses = []\n",
    "\n",
    "    # Check for symptom matches\n",
    "    for token in input_tokens:\n",
    "        if token in symptom_list:\n",
    "            detected_symptoms.append(token)\n",
    "    \n",
    "    # Check for diagnosis matches\n",
    "    for token in input_tokens:\n",
    "        if token in diagnosis_list:\n",
    "            detected_diagnoses.append(token)\n",
    "            \n",
    "    # Remove duplicates and return sorted lists\n",
    "    detected_symptoms = sorted(list(set(detected_symptoms)))\n",
    "    detected_diagnoses = sorted(list(set(detected_diagnoses)))\n",
    "\n",
    "    return detected_symptoms, detected_diagnoses\n",
    "\n",
    "print(\"Fungsi deteksi entitas medis `detect_medical_entities` telah didefinisikan.\")\n",
    "\n",
    "# --- Contoh Penggunaan Fungsi Deteksi Entitas ---\n",
    "print(\"\\n--- Contoh Deteksi Entitas ---\")\n",
    "contoh_input = \"saya mengalami demam tinggi dan sakit kepala yang hebat sekali\"\n",
    "symptoms, diagnoses = detect_medical_entities(contoh_input, unique_symptoms, unique_diagnosis)\n",
    "\n",
    "print(f\"Input: '{contoh_input}'\")\n",
    "print(f\"Gejala terdeteksi: {symptoms}\")\n",
    "print(f\"Diagnosis terdeteksi: {diagnoses}\")\n",
    "\n",
    "contoh_input_2 = \"kaku kuduk dan penurunan kesadaran itu adalah gejala meningitis\"\n",
    "symptoms_2, diagnoses_2 = detect_medical_entities(contoh_input_2, unique_symptoms, unique_diagnosis)\n",
    "\n",
    "print(f\"\\nInput: '{contoh_input_2}'\")\n",
    "print(f\"Gejala terdeteksi: {symptoms_2}\")\n",
    "print(f\"Diagnosis terdeteksi: {diagnoses_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "067c2633-c4d1-4641-b39f-29034e24a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from: ../models/medical_bert/model/\n",
      "Loading model from: ../models/medical_bert/model/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedicalBERT tokenizer and model loaded successfully.\n",
      "Fungsi `bert_ner` untuk deteksi entitas menggunakan MedicalBERT telah didefinisikan.\n",
      "\n",
      "--- Contoh Deteksi Entitas dengan MedicalBERT ---\n",
      "Input: 'pasien mengalami demam tinggi dan batuk berdahak'\n",
      "Output BERT (Detected Entities):\n",
      "{'word': 'pasien', 'entity_type': 'LABEL_2'}\n",
      "{'word': 'demam', 'entity_type': 'LABEL_2'}\n",
      "{'word': 'tinggi', 'entity_type': 'LABEL_2'}\n",
      "{'word': 'dan', 'entity_type': 'LABEL_2'}\n",
      "{'word': 'batuk', 'entity_type': 'LABEL_2'}\n",
      "{'word': 'berdahak', 'entity_type': 'LABEL_1'}\n",
      "\n",
      "Input: 'diagnosis adalah meningitis dan terjadi perdarahan subarachnoid'\n",
      "Output BERT (Detected Entities):\n",
      "{'word': 'diagnosis', 'entity_type': 'LABEL_2'}\n",
      "{'word': 'adalah', 'entity_type': 'LABEL_1'}\n",
      "{'word': 'meningitis', 'entity_type': 'LABEL_2'}\n",
      "{'word': 'dan', 'entity_type': 'LABEL_2'}\n",
      "{'word': 'perdarahan', 'entity_type': 'LABEL_2'}\n",
      "{'word': 'subarachnoid', 'entity_type': 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained tokenizer and model (assuming they are saved locally)\n",
    "# Adjust 'path_to_medical_bert_model' if you saved it elsewhere\n",
    "path_to_medical_bert_model = \"../models/medical_bert/model/\"\n",
    "\n",
    "print(f\"Loading tokenizer from: {path_to_medical_bert_model}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path_to_medical_bert_model)\n",
    "    print(f\"Loading model from: {path_to_medical_bert_model}\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(path_to_medical_bert_model)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    print(\"MedicalBERT tokenizer and model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading MedicalBERT model or tokenizer: {e}\")\n",
    "    print(\"Please ensure the model files are correctly located at the specified path.\")\n",
    "    # Exit or handle error gracefully if loading fails\n",
    "    tokenizer = None\n",
    "    model = None\n",
    "\n",
    "\n",
    "def bert_ner(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Performs Named Entity Recognition (NER) using a pre-trained BERT model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to analyze.\n",
    "        tokenizer: The pre-trained tokenizer (e.g., AutoTokenizer).\n",
    "        model: The pre-trained model for token classification (e.g., AutoModelForTokenClassification).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a detected entity\n",
    "              with 'word', 'entity_type', and 'score'.\n",
    "              The entity_type here will be based on the model's internal labels.\n",
    "    \"\"\"\n",
    "    if tokenizer is None or model is None:\n",
    "        return [] # Return empty if model/tokenizer failed to load\n",
    "\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get predictions (logits) and convert to probabilities\n",
    "    predictions = torch.softmax(outputs.logits, dim=2).squeeze(0)\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "\n",
    "    # Get original tokens and map predicted labels to actual entity tags\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze(0))\n",
    "    \n",
    "    # Map label IDs to their string names (e.g., 0 -> 'O', 1 -> 'B-SYMPTOM')\n",
    "    # This relies on the model's config.id2label dictionary\n",
    "    id2label = model.config.id2label if hasattr(model.config, 'id2label') else {i: f\"LABEL_{i}\" for i in range(model.config.num_labels)}\n",
    "\n",
    "    # Process tokens and labels to extract entities\n",
    "    detected_entities = []\n",
    "    \n",
    "    # Iterate through tokens and their predicted labels\n",
    "    for i, (token_id, label_id) in enumerate(zip(inputs[\"input_ids\"].squeeze(0), predicted_labels)):\n",
    "        token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "        label = id2label.get(label_id.item(), \"UNKNOWN\") # Get label string\n",
    "\n",
    "        # Skip special tokens ([CLS], [SEP], [PAD])\n",
    "        if token in tokenizer.all_special_tokens:\n",
    "            continue\n",
    "        \n",
    "        # Reconstruct original words from WordPiece tokens\n",
    "        if token.startswith(\"##\"):\n",
    "            # If the previous token was part of an entity, append this subword\n",
    "            if detected_entities and detected_entities[-1]['end_idx'] == i - 1: # Check if contiguous\n",
    "                detected_entities[-1]['word'] += token[2:]\n",
    "                detected_entities[-1]['end_idx'] = i # Update end index\n",
    "            else: # Should not happen if it's a continuation, but as fallback\n",
    "                detected_entities.append({\"word\": token[2:], \"entity_type\": label, \"start_idx\": i, \"end_idx\": i})\n",
    "        else:\n",
    "            detected_entities.append({\"word\": token, \"entity_type\": label, \"start_idx\": i, \"end_idx\": i})\n",
    "            \n",
    "    # Refine and filter entities based on common NER patterns (B-I-O)\n",
    "    # This part depends heavily on the specific NER labels of your MedicalBERT model.\n",
    "    # For a general BERT model not fine-tuned for medical NER, most labels might be 'O'.\n",
    "    # If it *is* fine-tuned, labels might be 'B-SYMPTOM', 'I-SYMPTOM', 'B-DISEASE', etc.\n",
    "    \n",
    "    # For this exercise, we will just return tokens that are not 'O' (Outside)\n",
    "    # This assumes 'O' is label ID 0, which is common. Check model.config.id2label.\n",
    "    final_entities = []\n",
    "    current_entity_word = \"\"\n",
    "    current_entity_type = \"\"\n",
    "\n",
    "    for item in detected_entities:\n",
    "        word = item['word']\n",
    "        entity_type = item['entity_type']\n",
    "        \n",
    "        # If the label is 'O' or UNKNOWN, it's not a recognized entity part\n",
    "        if entity_type == 'O' or entity_type.startswith('LABEL_0') or entity_type == 'UNKNOWN': # Assuming 'O' is label 0\n",
    "            # If we had a current entity, add it to final_entities and reset\n",
    "            if current_entity_word:\n",
    "                final_entities.append({'word': current_entity_word.strip(), 'entity_type': current_entity_type})\n",
    "                current_entity_word = \"\"\n",
    "                current_entity_type = \"\"\n",
    "            continue\n",
    "            \n",
    "        # Handle B- (Beginning) and I- (Inside) tags for multi-word entities\n",
    "        if entity_type.startswith('B-'):\n",
    "            if current_entity_word: # If there was a previous entity being built, finalize it\n",
    "                final_entities.append({'word': current_entity_word.strip(), 'entity_type': current_entity_type})\n",
    "            current_entity_word = word\n",
    "            current_entity_type = entity_type[2:] # Remove 'B-' prefix\n",
    "        elif entity_type.startswith('I-'):\n",
    "            # Ensure it's a continuation of the same type\n",
    "            if current_entity_word and entity_type[2:] == current_entity_type:\n",
    "                current_entity_word += \" \" + word\n",
    "            else: # Malformed sequence or new single-word I-tag (treat as B- for simplicity here)\n",
    "                if current_entity_word: # Add previous entity if it exists\n",
    "                    final_entities.append({'word': current_entity_word.strip(), 'entity_type': current_entity_type})\n",
    "                current_entity_word = word\n",
    "                current_entity_type = entity_type[2:]\n",
    "        else: # Single-word entities or unexpected tags (treat as individual entity)\n",
    "            if current_entity_word: # Add previous entity if it exists\n",
    "                final_entities.append({'word': current_entity_word.strip(), 'entity_type': current_entity_type})\n",
    "            current_entity_word = word\n",
    "            current_entity_type = entity_type # Use the label as is\n",
    "\n",
    "    # Add any remaining entity\n",
    "    if current_entity_word:\n",
    "        final_entities.append({'word': current_entity_word.strip(), 'entity_type': current_entity_type})\n",
    "\n",
    "    return final_entities\n",
    "\n",
    "\n",
    "print(\"Fungsi `bert_ner` untuk deteksi entitas menggunakan MedicalBERT telah didefinisikan.\")\n",
    "\n",
    "# --- Contoh Penggunaan Fungsi Deteksi Entitas dengan MedicalBERT ---\n",
    "print(\"\\n--- Contoh Deteksi Entitas dengan MedicalBERT ---\")\n",
    "contoh_input_bert = \"pasien mengalami demam tinggi dan batuk berdahak\"\n",
    "bert_output = bert_ner(contoh_input_bert, tokenizer, model)\n",
    "\n",
    "print(f\"Input: '{contoh_input_bert}'\")\n",
    "print(\"Output BERT (Detected Entities):\")\n",
    "for item in bert_output:\n",
    "    print(item)\n",
    "\n",
    "# Contoh lain\n",
    "contoh_input_bert_2 = \"diagnosis adalah meningitis dan terjadi perdarahan subarachnoid\"\n",
    "bert_output_2 = bert_ner(contoh_input_bert_2, tokenizer, model)\n",
    "\n",
    "print(f\"\\nInput: '{contoh_input_bert_2}'\")\n",
    "print(\"Output BERT (Detected Entities):\")\n",
    "for item in bert_output_2:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88de2a73-cbf3-4c97-aacb-0d2ce10e5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fungsi respons chatbot `get_response` telah didefinisikan.\n",
      "\n",
      "--- Contoh Interaksi Chatbot ---\n",
      "\n",
      "Anda: Saya demam dan batuk berdahak.\n",
      "Bot: Saya mendeteksi gejala: batuk, demam. Saya mendeteksi diagnosis: batuk. Berdasarkan informasi yang Anda berikan, saya menduga ini terkait dengan: **nan**. Beberapa gejala terkait penyakit ini: Demam, menggigil, suhu tubuh meningkat, batuk berdahak kadang disertai darah, sesak nafas, nyeri dada.\n",
      "\n",
      "Anda: Gejala saya adalah sakit kepala dan muntah.\n",
      "Bot: Saya mendeteksi gejala: gejala, kepala, muntah, sakit. Saya mendeteksi diagnosis: sakit. Berdasarkan informasi yang Anda berikan, saya menduga ini terkait dengan: **Meningitis + perdarahan subarachnoid**. Beberapa gejala terkait penyakit ini: Kaku kuduk, penurunan kesadaran, muntah proyektil, sakit kepala.\n",
      "\n",
      "Anda: Apakah itu meningitis?\n",
      "Bot: Saya mendeteksi diagnosis: meningitis. Berdasarkan informasi yang Anda berikan, saya menduga ini terkait dengan: **Meningitis + perdarahan subarachnoid**. Beberapa gejala terkait penyakit ini: Kaku kuduk, penurunan kesadaran, muntah proyektil, sakit kepala.\n",
      "\n",
      "Anda: Saya tidak tahu.\n",
      "Bot: Saya tidak dapat menemukan diagnosis spesifik berdasarkan gejala yang Anda sebutkan.\n",
      "\n",
      "Anda: Kepala saya terasa pusing sekali dan mual.\n",
      "Bot: Saya mendeteksi gejala: kepala, mual. Berdasarkan informasi yang Anda berikan, saya menduga ini terkait dengan: **Botulismus**. Beberapa gejala terkait penyakit ini: mual, muntah, kram perut, mulut kering dan diare. Gejala saraf:pandangan kabur, kelopak mata jatuh, fotofobia, kekakuan sendi, gangguan bicara dan tidak dapat menelan makanan. Kelemahan otot (otot penggerak kepala, otot lengan atas, otot pernapasan, otot tungkai bagian bawah..\n"
     ]
    }
   ],
   "source": [
    "# Ensure df_penyakit, unique_symptoms, unique_diagnosis, tokenizer, and model (MedicalBERT)\n",
    "# are loaded and defined from previous cells.\n",
    "\n",
    "def get_response(user_input, df, unique_symptoms, unique_diagnosis, bert_tokenizer, bert_model):\n",
    "    \"\"\"\n",
    "    Generates a chatbot response based on user input, leveraging both keyword and BERT-based entity detection.\n",
    "\n",
    "    Args:\n",
    "        user_input (str): Raw user query.\n",
    "        df (pd.DataFrame): DataFrame with 'gejala_processed' and 'diagnosis_processed' columns.\n",
    "        unique_symptoms (list): List of unique preprocessed symptom words.\n",
    "        unique_diagnosis (list): List of unique preprocessed diagnosis words.\n",
    "        bert_tokenizer: Pre-trained BERT tokenizer.\n",
    "        bert_model: Pre-trained BERT model for token classification.\n",
    "\n",
    "    Returns:\n",
    "        str: The chatbot's response.\n",
    "    \"\"\"\n",
    "    if not user_input.strip():\n",
    "        return \"Halo! Ada yang bisa saya bantu terkait kesehatan Anda?\"\n",
    "\n",
    "    # --- Step 1: Keyword-based Entity Detection ---\n",
    "    # Detect entities using the lookup table method.\n",
    "    detected_symptoms_kw, detected_diagnoses_kw = detect_medical_entities(\n",
    "        user_input, unique_symptoms, unique_diagnosis\n",
    "    )\n",
    "    \n",
    "    # --- Step 2: BERT-based Entity Detection ---\n",
    "    # Detect entities using the MedicalBERT model for contextual understanding.\n",
    "    bert_entities = bert_ner(user_input, bert_tokenizer, bert_model)\n",
    "    \n",
    "    # Combine results. Prioritize keyword matches and add relevant BERT detections.\n",
    "    # Collect all detected entities from both methods.\n",
    "    all_detected_keywords = set(detected_symptoms_kw + detected_diagnoses_kw)\n",
    "    \n",
    "    for entity in bert_entities:\n",
    "        # Check if the BERT-detected entity type is not 'O' (Outside) or a default unassigned label.\n",
    "        # This assumes LABEL_1, LABEL_2, etc., represent actual medical entities in your BERT model.\n",
    "        if entity['entity_type'] != 'O' and not entity['entity_type'].startswith('LABEL_0'):\n",
    "            # Add BERT-detected words if they are meaningful (more than 1 character)\n",
    "            # and not already present from the keyword lookup.\n",
    "            if len(entity['word']) > 1 and entity['word'] not in all_detected_keywords:\n",
    "                all_detected_keywords.add(entity['word'])\n",
    "\n",
    "    all_detected_keywords = list(all_detected_keywords)\n",
    "    \n",
    "    response_parts = []\n",
    "\n",
    "    if detected_symptoms_kw:\n",
    "        response_parts.append(f\"Saya mendeteksi gejala: {', '.join(detected_symptoms_kw)}.\")\n",
    "    \n",
    "    if detected_diagnoses_kw:\n",
    "        response_parts.append(f\"Saya mendeteksi diagnosis: {', '.join(detected_diagnoses_kw)}.\")\n",
    "\n",
    "    if all_detected_keywords:\n",
    "        # Attempt to find the most relevant row in the DataFrame.\n",
    "        # This part could use TF-IDF or BERT Embeddings for more advanced similarity,\n",
    "        # but for a quick demo, we'll use keyword matching on 'gejala_processed' and 'diagnosis_processed' columns.\n",
    "\n",
    "        relevant_rows = []\n",
    "        for index, row in df.iterrows():\n",
    "            gejala_text = row['gejala_processed']\n",
    "            diagnosis_text = row['diagnosis_processed']\n",
    "            \n",
    "            # Count how many keywords match in the processed symptom or diagnosis texts.\n",
    "            match_count = 0\n",
    "            for keyword in all_detected_keywords:\n",
    "                if keyword in gejala_text.split() or keyword in diagnosis_text.split():\n",
    "                    match_count += 1\n",
    "            \n",
    "            # If there's any match, store the row along with the match count.\n",
    "            if match_count > 0:\n",
    "                relevant_rows.append({'row': row, 'matches': match_count})\n",
    "        \n",
    "        # Sort by the highest number of matches.\n",
    "        relevant_rows = sorted(relevant_rows, key=lambda x: x['matches'], reverse=True)\n",
    "\n",
    "        if relevant_rows:\n",
    "            top_match_row = relevant_rows[0]['row']\n",
    "            response_parts.append(f\"Berdasarkan informasi yang Anda berikan, saya menduga ini terkait dengan: **{top_match_row['diagnosis']}**.\")\n",
    "            response_parts.append(f\"Beberapa gejala terkait penyakit ini: {top_match_row['penyakit']}.\")\n",
    "        else:\n",
    "            response_parts.append(\"Saya tidak dapat menemukan diagnosis spesifik berdasarkan gejala yang Anda sebutkan.\")\n",
    "    else:\n",
    "        response_parts.append(\"Mohon berikan detail lebih lanjut tentang gejala atau kondisi Anda.\")\n",
    "\n",
    "    final_response = \" \".join(response_parts)\n",
    "    return final_response if final_response else \"Maaf, saya tidak memahami pertanyaan Anda. Bisakah Anda mengulanginya dengan lebih jelas?\"\n",
    "\n",
    "print(\"Fungsi respons chatbot `get_response` telah didefinisikan.\")\n",
    "\n",
    "# --- Contoh Interaksi Chatbot ---\n",
    "print(\"\\n--- Contoh Interaksi Chatbot ---\")\n",
    "chat_history = []\n",
    "\n",
    "def chat_with_bot(message):\n",
    "    print(f\"\\nAnda: {message}\")\n",
    "    response = get_response(message, df_penyakit, unique_symptoms, unique_diagnosis, tokenizer, model)\n",
    "    print(f\"Bot: {response}\")\n",
    "    chat_history.append({\"user\": message, \"bot\": response})\n",
    "    \n",
    "# Contoh 1\n",
    "chat_with_bot(\"Saya demam dan batuk berdahak.\")\n",
    "\n",
    "# Contoh 2\n",
    "chat_with_bot(\"Gejala saya adalah sakit kepala dan muntah.\")\n",
    "\n",
    "# Contoh 3\n",
    "chat_with_bot(\"Apakah itu meningitis?\")\n",
    "\n",
    "# Contoh 4\n",
    "chat_with_bot(\"Saya tidak tahu.\")\n",
    "\n",
    "# Example 5 (testing BERT's ability for words not in the lookup table, but possibly related)\n",
    "# This depends on how well your BERT model is trained for medical NER.\n",
    "# If your BERT is not specifically trained for medical NER, it may not make much difference.\n",
    "chat_with_bot(\"Kepala saya terasa pusing sekali dan mual.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc9189-bf71-4549-8bc2-2d77197296fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
